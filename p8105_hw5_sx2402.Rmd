---
title: "p8105_hw5_sx2402"
author: "Eric Xu"
date: "2025-11-02"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(skimr)
library(purrr)
```

## Problem 1

1. Creating a function generating random birthdays.

```{r function1}
has_shared_birthday = function(n) {
  if (!is.numeric(n)) {
    stop("Argument n should be numeric")
  } else {
    birthdays = sample(1:365, size = n, replace = TRUE)
    return(length(unique(birthdays)) < n)
  }
}
```

2. Simulating and plotting.

```{r Q1 run, cache = TRUE}
n_sim = 10000
group_sizes = 2:50
results = tibble()
for (n in group_sizes) {
  shared_vector = logical(n_sim)
  
  for (i in 1:n_sim) {
    shared_vector[i] = has_shared_birthday(n)
  }
  prob_shared = mean(shared_vector)
  
  results = bind_rows(results, tibble(group_size = n, prob_shared = prob_shared))
}

results |> ggplot(aes(x=group_size, y = prob_shared))+geom_point(size = 2, color = "black")+geom_smooth(method = 'loess',formula = y~x, alpha = 0.8, linewidth = 0.6)
```

3. Comment

The group size and probability of shared birthday shows a sigmoid curve that with increase in group size, the probability of having a replicated birthday is increasing in a cumulative probability distribution. As the group size increases, the probability rises non linearly with increasing pairs, accelerating rapidly around 20–30 people.

## Problem 2
1. Creating a function generating random normal distribution values.

```{r function2}
sim_ttest = function(mu_true, n = 30, sigma = 5, n_sim = 5000) {
  result = vector("list",5000)
  for (i in 1:n_sim){
      x = rnorm(n, mean = mu_true, sd = sigma)
      ttest = broom::tidy(t.test(x, mu = 0,conf.level = 0.95))
      run = tibble(
      mu_hat = ttest$estimate,
      p_value = ttest$p.value
  )
  result[[i]] = run
  }
  bind_rows(result)
}
```

2. Simulation

```{r Q2 sim, cache = TRUE}
sim_results_df = 
  expand_grid(
    mu_values = 0:6
  ) |> 
  mutate(
    estimate_df = map(mu_values, sim_ttest)
  ) |> 
  unnest(estimate_df)

power_results = sim_results_df |> 
  group_by(mu_values) |> 
  summarize(power = mean(p_value < 0.05))


ggplot(power_results, aes(x = mu_values, y = power)) +
  geom_line(linewidth = 1.2, color = "steelblue") +
  geom_point(size = 2, color = "darkred") +
  labs(
    title = "Power Curve for One-Sample t-Test",
    x = "True Mean (μ)",
    y = "Power by reject proportion"
  ) +
  theme_minimal()


mu_summary = sim_results_df |> 
  group_by(mu_values) |> 
  summarize(
    mean_mu_select = mean(mu_hat[p_value < 0.05]),
    mu_hat_mean = mean(mu_hat),
  )

ggplot(mu_summary, aes(x = mu_values)) +
  geom_line(aes(y = mu_hat_mean, color = "All samples")) +
  geom_point(aes(y = mu_hat_mean, color = "All samples")) +
  geom_line(aes(y = mean_mu_select, color = "Significant only")) +
  geom_point(aes(y = mean_mu_select, color = "Significant only")) +
  labs(
    title = "Average Estimates vs True mu",
    x = "True Mean",
    y = "Average Estimate",
    color = "Condition"
  ) +
  scale_color_manual(values = c("All samples" = "steelblue", "Significant only" = "darkred")) +
  theme_minimal()
```

3. Explanation

* The power Curve for t-test shows a sigmoid distribution from 0 to 1. This is seemingly resulted by cumulative probability with increase of true mu.

* When  condition on rejecting the null, selecting samples causes bias. For small true effects , most samples won’t reach significance. with True Mean close to 0, a proportion of estimates has hypothesis not rejected. The filtered set will likely to have higher mean estimates that resulting in biased average estimates. Therefore, for which the null is rejected is not approximately equal to the true value compared to all samples included at a lower effect from 1 to 4. And for 0 a smaller number of sets falsely rejected the null.

## Problem 3

```{r Q3 import and function, collapse = TRUE}
homicide_data = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv") |> 
janitor::clean_names()


homicide_summary = homicide_data %>%
  mutate(
    city_state = str_c(city, ", ", state)
  ) %>%
  group_by(city_state) %>%
  summarize(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

prop_test = function(name) {
  city = homicide_summary |> filter(city_state == name)
  if (!(name %in% city$city_state)){
    stop("No given city found.")
  }
  if (city$total < 30) {
    short_ptest = tibble(
      CI = NA_character_,
      estimate = NA_real_
    )
  } else {
    prop_test_result = broom::tidy(prop.test(city$unsolved, city$total))
    
    short_ptest = tibble(
      CI = str_c(
        round(prop_test_result$conf.low, 3), ", ", round(prop_test_result$conf.high, 3)
      ),
      estimate = round(prop_test_result$estimate, 4)
    )
  }
  
  return(short_ptest)
}
```

1. Testing Baltimore and all other cities.

```{r testing data}
prop_test("Baltimore, MD")

homi_result = tibble()
homi_result = homicide_summary %>%
  mutate(
    results = map(city_state, prop_test)
  ) %>%
  unnest(results) |> 
  arrange(desc(estimate))

kable(homi_result)
```

2. Plotting (Removed one city that has 0 unresolved over 1, not available for testing)

```{r plot Q3, fig.width=12, fig.height=10, dpi = 900}
homi_plot = homi_result |> 
  separate(CI, into = c("ci_low", "ci_high"), sep = ", ", convert = TRUE) |> 
  arrange(desc(estimate))


homi_plot <- homi_plot |> 
  filter(!is.na(estimate)) |> 
  mutate(city_state = forcats::fct_reorder(city_state, estimate))

homi_plot = homi_plot|> 
  ggplot(aes(x = estimate, y = city_state)) +
  geom_errorbar(aes(xmin = ci_low, xmax = ci_high), na.rm =  TRUE, color = "grey60", linewidth = 0.9) +
  geom_point(size = 1.3, na.rm = TRUE, color = "orange") +
  labs(
    title = "Estimated Proportion of Unsolved Homicides by City",
    x = "Proportion of Unsolved Homicides (95% CI)",
    y = NULL
  ) +
  theme_minimal()
homi_plot

```

