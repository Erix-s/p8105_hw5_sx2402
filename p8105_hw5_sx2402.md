p8105_hw5_sx2402
================
Eric Xu
2025-11-02

## Problem 1

1.  Creating a function generating random birthdays.

``` r
has_shared_birthday = function(n) {
  if (!is.numeric(n)) {
    stop("Argument n should be numeric")
  } else {
    birthdays = sample(1:365, size = n, replace = TRUE)
    return(length(unique(birthdays)) < n)
  }
}
```

2.  Simulating and plotting.

``` r
n_sim = 10000
group_sizes = 2:50
results = tibble()
for (n in group_sizes) {
  shared_vector = logical(n_sim)
  
  for (i in 1:n_sim) {
    shared_vector[i] = has_shared_birthday(n)
  }
  prob_shared = mean(shared_vector)
  
  results = bind_rows(results, tibble(group_size = n, prob_shared = prob_shared))
}

results |> ggplot(aes(x=group_size, y = prob_shared))+geom_point(size = 2, color = "black")+geom_smooth(method = 'loess',formula = y~x, alpha = 0.8, linewidth = 0.6)
```

![](p8105_hw5_sx2402_files/figure-gfm/Q1%20run-1.png)<!-- -->

3.  Comment

The group size and probability of shared birthday shows a sigmoid curve
that with increase in group size, the probability of having a replicated
birthday is increasing in a cumulative probability distribution. As the
group size increases, the probability rises non linearly with increasing
pairs, accelerating rapidly around 20–30 people.

## Problem 2

1.  Creating a function generating random normal distribution values.

``` r
sim_ttest = function(mu_true, n = 30, sigma = 5, n_sim = 5000) {
  result = vector("list",5000)
  for (i in 1:n_sim){
      x = rnorm(n, mean = mu_true, sd = sigma)
      ttest = broom::tidy(t.test(x, mu = 0,conf.level = 0.95))
      run = tibble(
      mu_hat = ttest$estimate,
      p_value = ttest$p.value
  )
  result[[i]] = run
  }
  bind_rows(result)
}
```

2.  Simulation

``` r
sim_results_df = 
  expand_grid(
    mu_values = 0:6
  ) |> 
  mutate(
    estimate_df = map(mu_values, sim_ttest)
  ) |> 
  unnest(estimate_df)

power_results = sim_results_df |> 
  group_by(mu_values) |> 
  summarize(power = mean(p_value < 0.05))


ggplot(power_results, aes(x = mu_values, y = power)) +
  geom_line(linewidth = 1.2, color = "steelblue") +
  geom_point(size = 2, color = "darkred") +
  labs(
    title = "Power Curve for One-Sample t-Test",
    x = "True Mean (μ)",
    y = "Power by reject proportion"
  ) +
  theme_minimal()
```

![](p8105_hw5_sx2402_files/figure-gfm/Q2%20sim-1.png)<!-- -->

``` r
mu_summary = sim_results_df |> 
  group_by(mu_values) |> 
  summarize(
    mean_mu_select = mean(mu_hat[p_value < 0.05]),
    mu_hat_mean = mean(mu_hat),
  )

ggplot(mu_summary, aes(x = mu_values)) +
  geom_line(aes(y = mu_hat_mean, color = "All samples")) +
  geom_point(aes(y = mu_hat_mean, color = "All samples")) +
  geom_line(aes(y = mean_mu_select, color = "Significant only")) +
  geom_point(aes(y = mean_mu_select, color = "Significant only")) +
  labs(
    title = "Average Estimates vs True mu",
    x = "True Mean",
    y = "Average Estimate",
    color = "Condition"
  ) +
  scale_color_manual(values = c("All samples" = "steelblue", "Significant only" = "darkred")) +
  theme_minimal()
```

![](p8105_hw5_sx2402_files/figure-gfm/Q2%20sim-2.png)<!-- -->

3.  Explanation

- The power Curve for t-test shows a sigmoid distribution from 0 to 1.
  This is seemingly resulted by cumulative probability with increase of
  true mu.

- When condition on rejecting the null, selecting samples causes bias.
  For small true effects , most samples won’t reach significance. with
  True Mean close to 0, a proportion of estimates has hypothesis not
  rejected. The filtered set will likely to have higher mean estimates
  that resulting in biased average estimates. Therefore, for which the
  null is rejected is not approximately equal to the true value compared
  to all samples included at a lower effect from 1 to 4. And for 0 a
  smaller number of sets falsely rejected the null.

## Problem 3

``` r
homicide_data = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv") |> 
janitor::clean_names()
## Rows: 52179 Columns: 12
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: ","
## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
## dbl (3): reported_date, lat, lon
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.


homicide_summary = homicide_data %>%
  mutate(
    city_state = str_c(city, ", ", state)
  ) %>%
  group_by(city_state) %>%
  summarize(
    total = n(),
    unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  )

prop_test = function(name) {
  city = homicide_summary |> filter(city_state == name)
  if (!(name %in% city$city_state)){
    stop("No given city found.")
  }
  if (city$total < 30) {
    short_ptest = tibble(
      CI = NA_character_,
      estimate = NA_real_
    )
  } else {
    prop_test_result = broom::tidy(prop.test(city$unsolved, city$total))
    
    short_ptest = tibble(
      CI = str_c(
        round(prop_test_result$conf.low, 3), ", ", round(prop_test_result$conf.high, 3)
      ),
      estimate = round(prop_test_result$estimate, 4)
    )
  }
  
  return(short_ptest)
}
```

1.  Testing Baltimore and all other cities.

``` r
prop_test("Baltimore, MD")
```

    ## # A tibble: 1 × 2
    ##   CI           estimate
    ##   <chr>           <dbl>
    ## 1 0.628, 0.663    0.646

``` r
homi_result = tibble()
homi_result = homicide_summary %>%
  mutate(
    results = map(city_state, prop_test)
  ) %>%
  unnest(results) |> 
  arrange(desc(estimate))

kable(homi_result)
```

| city_state         | total | unsolved | CI           | estimate |
|:-------------------|------:|---------:|:-------------|---------:|
| Chicago, IL        |  5535 |     4073 | 0.724, 0.747 |   0.7359 |
| New Orleans, LA    |  1434 |      930 | 0.623, 0.673 |   0.6485 |
| Baltimore, MD      |  2827 |     1825 | 0.628, 0.663 |   0.6456 |
| San Bernardino, CA |   275 |      170 | 0.558, 0.675 |   0.6182 |
| Buffalo, NY        |   521 |      319 | 0.569, 0.654 |   0.6123 |
| Miami, FL          |   744 |      450 | 0.569, 0.64  |   0.6048 |
| Stockton, CA       |   444 |      266 | 0.552, 0.645 |   0.5991 |
| Detroit, MI        |  2519 |     1482 | 0.569, 0.608 |   0.5883 |
| Phoenix, AZ        |   914 |      504 | 0.518, 0.584 |   0.5514 |
| Denver, CO         |   312 |      169 | 0.485, 0.598 |   0.5417 |
| St. Louis, MO      |  1677 |      905 | 0.515, 0.564 |   0.5397 |
| Oakland, CA        |   947 |      508 | 0.504, 0.569 |   0.5364 |
| Pittsburgh, PA     |   631 |      337 | 0.494, 0.573 |   0.5341 |
| Columbus, OH       |  1084 |      575 | 0.5, 0.56    |   0.5304 |
| Jacksonville, FL   |  1168 |      597 | 0.482, 0.54  |   0.5111 |
| Minneapolis, MN    |   366 |      187 | 0.459, 0.563 |   0.5109 |
| Houston, TX        |  2942 |     1493 | 0.489, 0.526 |   0.5075 |
| San Francisco, CA  |   663 |      336 | 0.468, 0.545 |   0.5068 |
| Boston, MA         |   614 |      310 | 0.465, 0.545 |   0.5049 |
| Los Angeles, CA    |  2257 |     1106 | 0.469, 0.511 |   0.4900 |
| Oklahoma City, OK  |   672 |      326 | 0.447, 0.524 |   0.4851 |
| Dallas, TX         |  1567 |      754 | 0.456, 0.506 |   0.4812 |
| Savannah, GA       |   246 |      115 | 0.404, 0.532 |   0.4675 |
| Fort Worth, TX     |   549 |      255 | 0.422, 0.507 |   0.4645 |
| Baton Rouge, LA    |   424 |      196 | 0.414, 0.511 |   0.4623 |
| Tampa, FL          |   208 |       95 | 0.388, 0.527 |   0.4567 |
| Louisville, KY     |   576 |      261 | 0.412, 0.495 |   0.4531 |
| Indianapolis, IN   |  1322 |      594 | 0.422, 0.477 |   0.4493 |
| Philadelphia, PA   |  3037 |     1360 | 0.43, 0.466  |   0.4478 |
| Cincinnati, OH     |   694 |      309 | 0.408, 0.483 |   0.4452 |
| Washington, DC     |  1345 |      589 | 0.411, 0.465 |   0.4379 |
| Birmingham, AL     |   800 |      347 | 0.399, 0.469 |   0.4338 |
| San Antonio, TX    |   833 |      357 | 0.395, 0.463 |   0.4286 |
| Las Vegas, NV      |  1381 |      572 | 0.388, 0.441 |   0.4142 |
| Omaha, NE          |   409 |      169 | 0.365, 0.463 |   0.4132 |
| Long Beach, CA     |   378 |      156 | 0.363, 0.464 |   0.4127 |
| Kansas City, MO    |  1190 |      486 | 0.38, 0.437  |   0.4084 |
| New York, NY       |   627 |      243 | 0.349, 0.427 |   0.3876 |
| Albuquerque, NM    |   378 |      146 | 0.337, 0.438 |   0.3862 |
| Atlanta, GA        |   973 |      373 | 0.353, 0.415 |   0.3834 |
| San Diego, CA      |   461 |      175 | 0.335, 0.426 |   0.3796 |
| Sacramento, CA     |   376 |      139 | 0.321, 0.421 |   0.3697 |
| Durham, NC         |   276 |      101 | 0.31, 0.426  |   0.3659 |
| Nashville, TN      |   767 |      278 | 0.329, 0.398 |   0.3625 |
| Milwaukee, wI      |  1115 |      403 | 0.333, 0.391 |   0.3614 |
| Fresno, CA         |   487 |      169 | 0.305, 0.391 |   0.3470 |
| Tulsa, OK          |   583 |      193 | 0.293, 0.371 |   0.3310 |
| Memphis, TN        |  1514 |      483 | 0.296, 0.343 |   0.3190 |
| Charlotte, NC      |   687 |      206 | 0.266, 0.336 |   0.2999 |
| Richmond, VA       |   429 |      113 | 0.223, 0.308 |   0.2634 |
| Tulsa, AL          |     1 |        0 | NA           |       NA |

2.  Plotting (Removed one city that has 0 unresolved over 1, not
    available for testing)

``` r
homi_plot = homi_result |> 
  separate(CI, into = c("ci_low", "ci_high"), sep = ", ", convert = TRUE) |> 
  arrange(desc(estimate))


homi_plot <- homi_plot |> 
  filter(!is.na(estimate)) |> 
  mutate(city_state = forcats::fct_reorder(city_state, estimate))

homi_plot = homi_plot|> 
  ggplot(aes(x = estimate, y = city_state)) +
  geom_errorbar(aes(xmin = ci_low, xmax = ci_high), na.rm =  TRUE, color = "grey60", linewidth = 0.9) +
  geom_point(size = 1.3, na.rm = TRUE, color = "orange") +
  labs(
    title = "Estimated Proportion of Unsolved Homicides by City",
    x = "Proportion of Unsolved Homicides (95% CI)",
    y = NULL
  ) +
  theme_minimal()
homi_plot
```

![](p8105_hw5_sx2402_files/figure-gfm/plot%20Q3-1.png)<!-- -->
